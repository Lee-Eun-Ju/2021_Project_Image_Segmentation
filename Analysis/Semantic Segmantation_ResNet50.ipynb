{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPKhboWlwvd+AJsJiI13VHJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jstCIYgK8QN","executionInfo":{"status":"ok","timestamp":1638852071001,"user_tz":-540,"elapsed":473,"user":{"displayName":"이은주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02259566661853209373"}},"outputId":"75e633c9-6715-472e-8b71-6aa8feecf26d"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"7cD-2-mHK90I","executionInfo":{"status":"ok","timestamp":1638852073157,"user_tz":-540,"elapsed":342,"user":{"displayName":"이은주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02259566661853209373"}}},"source":["dir_data = \"/content/gdrive/My Drive/Colab Notebooks/SML/dataset1\"\n","dir_seg = dir_data + \"/annotations_prepped_train/\"\n","dir_img = dir_data + \"/images_prepped_train/\"\n","\n","import glob, os\n","all_img_paths = glob.glob(os.path.join(dir_img, \"*.png\"))\n","all_mask_paths = glob.glob(os.path.join(dir_seg, \"*.png\"))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"aWAbhH7XK-DJ","executionInfo":{"status":"ok","timestamp":1638852110700,"user_tz":-540,"elapsed":35394,"user":{"displayName":"이은주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02259566661853209373"}}},"source":["import cv2\n","import imageio\n","\n","x = []\n","y = []\n","for i in range(len(all_img_paths)):\n","    img = cv2.imread(all_img_paths[i])\n","    img = cv2.resize(img, (224,224))\n","    mask_path = dir_seg + all_img_paths[i].split('/')[-1]\n","    img_mask = imageio.imread(mask_path)\n","    img_mask = cv2.resize(img_mask, (224,224))\n","    x.append(img)\n","    y.append(img_mask)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mLJPwKW_LDYA","executionInfo":{"status":"ok","timestamp":1638805741556,"user_tz":-540,"elapsed":4335,"user":{"displayName":"이은주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02259566661853209373"}},"outputId":"4f6cdfb4-e31e-4954-aeae-fd55ae49053a"},"source":["import numpy as np\n","set(np.array(y).flatten())\n","n_classes = len(set(np.array(y).flatten()))\n","print(n_classes)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12\n"]}]},{"cell_type":"code","metadata":{"id":"guSble_rLJCo"},"source":["def getsegnum(img):\n","  seg_labels = np.zeros((224,224,12))\n","  for c in range(12):\n","    seg_labels[:,:,c] = (img==c).astype(int)\n","  return seg_labels\n","\n","y2 = []\n","for i in range(len(y)):\n","  y2.append(getsegnum(y[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QANjIf0lLJNW","executionInfo":{"status":"ok","timestamp":1638805756591,"user_tz":-540,"elapsed":10762,"user":{"displayName":"이은주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02259566661853209373"}},"outputId":"8c7d4061-24e3-4b21-937c-7ed719f864af"},"source":["x = np.array(x)/255\n","y2 = np.array(y2)\n","print(x.shape, y2.shape)\n","print(np.unique(y2))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(367, 224, 224, 3) (367, 224, 224, 12)\n","[0. 1.]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":492},"id":"cMqafWmnJ704","executionInfo":{"status":"error","timestamp":1638805758636,"user_tz":-540,"elapsed":2049,"user":{"displayName":"이은주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02259566661853209373"}},"outputId":"d5fc158b-139d-4745-cb0b-fbd93e2ecd25"},"source":["from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications import ResNet50\n","\n","def conv_block(input, num_filters):\n","    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","\n","    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","\n","    return x\n","\n","def decoder_block(input, skip_features, num_filters):\n","    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n","    x = Concatenate()([x, skip_features])\n","    x = conv_block(x, num_filters)\n","    return x\n","\n","def build_resnet50_unet(inputs):\n","\n","    \"\"\" Pre-trained ResNet50 Model \"\"\"\n","    resnet50 = ResNet50(include_top=False, weights=\"imagenet\", input_shape=inputs)\n","    resnet50.trainable = False\n","\n","    \"\"\" Encoder \"\"\"\n","    s1 = resnet50.get_layer(\"input_1\").output           ## (512 x 512)\n","    s2 = resnet50.get_layer(\"conv1_relu\").output        ## (256 x 256)\n","    s3 = resnet50.get_layer(\"conv2_block3_out\").output  ## (128 x 128)\n","    s4 = resnet50.get_layer(\"conv3_block4_out\").output  ## (64 x 64)\n","\n","    \"\"\" Bridge \"\"\"\n","    b1 = resnet50.get_layer(\"conv4_block6_out\").output  ## (32 x 32)\n","\n","    \"\"\" Decoder \"\"\"\n","    d1 = decoder_block(b1, s4, 224)                     ## (64 x 64)\n","    d2 = decoder_block(d1, s3, 112)                     ## (128 x 128)\n","    d3 = decoder_block(d2, s2, 56)                     ## (256 x 256)\n","    d4 = decoder_block(d3, s1, 28)                      ## (512 x 512)\n","\n","    \"\"\" Output \"\"\"\n","    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n","\n","    model = Model(inputs, outputs, name=\"ResNet50_U-Net\")\n","    return model\n","\n","if __name__ == \"__main__\":\n","    inputs = (224, 224, 3)\n","    model = build_resnet50_unet(inputs)\n","    model.summary()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-45a1bf3ef087>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_resnet50_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-45a1bf3ef087>\u001b[0m in \u001b[0;36mbuild_resnet50_unet\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\" Encoder \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m           \u001b[0;31m## (512 x 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1_relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m        \u001b[0;31m## (256 x 256)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0ms3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv2_block3_out\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m  \u001b[0;31m## (128 x 128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m   2629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2631\u001b[0;31m       raise ValueError(f'No such layer: {name}. Existing layers are '\n\u001b[0m\u001b[1;32m   2632\u001b[0m                        f'{self.layers}.')\n\u001b[1;32m   2633\u001b[0m     raise ValueError('Provide either a layer name or layer index at '\n","\u001b[0;31mValueError\u001b[0m: No such layer: input_1. Existing layers are [<keras.engine.input_layer.InputLayer object at 0x7f118bb78110>, <keras.layers.convolutional.ZeroPadding2D object at 0x7f118c3a6f10>, <keras.layers.convolutional.Conv2D object at 0x7f118c2a9d90>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f118b8b61d0>, <keras.layers.core.activation.Activation object at 0x7f118bb26090>, <keras.layers.convolutional.ZeroPadding2D object at 0x7f118c302a50>, <keras.layers.pooling.MaxPooling2D object at 0x7f118b922250>, <keras.layers.convolutional.Conv2D object at 0x7f1207739690>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f118c3948d0>, <keras.layers.core.activation.Activation object at 0x7f118bbf5910>, <keras.layers.convolutional.Conv2D object at 0x7f118c2d5dd0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f118bb88dd0>, <keras.layers.core.activation.Activation object at 0x7f1207725f90>, <keras.layers.convolutional.Conv2D object at 0x7f118c2e1d90>, <keras.layers.convolutional.Conv2D object at 0x7f118b715790>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f118b81d710>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f118ba56110>, <keras.layers.merge.Add object at 0x7f118bb881d0>, <keras.layers.core.activation.Activation object at 0x7f118b7dddd0>, <keras.layers.convolutional.Conv2D object at 0x7f118b7dd710>, ..."]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":437},"id":"orfWB3PJKX_y","executionInfo":{"status":"error","timestamp":1638804867101,"user_tz":-540,"elapsed":1826,"user":{"displayName":"이은주","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02259566661853209373"}},"outputId":"64eb363c-7400-4415-cdc8-017ac4e91f21"},"source":["model = build_resnet50_unet(inputs = (224, 224, 3))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-8b1af7ed101e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_resnet50_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-2bd1ee58c200>\u001b[0m in \u001b[0;36mbuild_resnet50_unet\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;34m\"\"\" Encoder \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m           \u001b[0;31m## (512 x 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1_relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m        \u001b[0;31m## (256 x 256)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0ms3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv2_block3_out\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m  \u001b[0;31m## (128 x 128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m   2629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2631\u001b[0;31m       raise ValueError(f'No such layer: {name}. Existing layers are '\n\u001b[0m\u001b[1;32m   2632\u001b[0m                        f'{self.layers}.')\n\u001b[1;32m   2633\u001b[0m     raise ValueError('Provide either a layer name or layer index at '\n","\u001b[0;31mValueError\u001b[0m: No such layer: input_1. Existing layers are [<keras.engine.input_layer.InputLayer object at 0x7f1187867a10>, <keras.layers.convolutional.ZeroPadding2D object at 0x7f118c563690>, <keras.layers.convolutional.Conv2D object at 0x7f118b6692d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f118c563490>, <keras.layers.core.activation.Activation object at 0x7f118ba8f790>, <keras.layers.convolutional.ZeroPadding2D object at 0x7f118b98db10>, <keras.layers.pooling.MaxPooling2D object at 0x7f118bbb5310>, <keras.layers.convolutional.Conv2D object at 0x7f118bca5bd0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f118c1d5650>, <keras.layers.core.activation.Activation object at 0x7f118c31b990>, <keras.layers.convolutional.Conv2D object at 0x7f118c31b9d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f118c31b710>, <keras.layers.core.activation.Activation object at 0x7f118bbb5e10>, <keras.layers.convolutional.Conv2D object at 0x7f118c4d7d50>, <keras.layers.convolutional.Conv2D object at 0x7f118bd03450>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f118c1b8f90>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f118bc990d0>, <keras.layers.merge.Add object at 0x7f118c18f050>, <keras.layers.core.activation.Activation object at 0x7f118c19fd90>, <keras.layers.convolutional.Conv2D object at 0x7f118bc37250>, ..."]}]},{"cell_type":"code","metadata":{"id":"SiVRbPo_YDGj"},"source":["model.compile(optimizer=Adam(1e-3, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy']) #class : 12개의 channel\n","history = model.fit(x,y2, epochs=10, batch_size=1, validation_split=0.1)"],"execution_count":null,"outputs":[]}]}